# MapLibre Native Renderer Design

This document describes how the low level MapLibre Native renderer works, as of the end of 2024.  We will focus on the renderer itself, rather than the style sheets or the vector tile parsing, though we will touch on those as needed.

The question we'll answer here is "How do the renderer itself work."

## History of the Renderer

Without getting too specific, the history of the renderer is a bit like this.

Before it was MapLibre it was Mapbox and they developed an OpenGL renderer for the web and then mobile devices.  The mobile device types included iOS and Android and could both be covered by OpenGL ES.

MapLibre was formed after Mapbox took its later versions of the toolkit private.  This coincided, conveniently enough, with the introduction of Metal from Apple and the requirement that apps support it.

Having the Apple devices in this sort of nebulous state, working but deprecated, was a problem for MapLibre.  Thus the desire to fix it, and a big pile of money, resulted in the Metal port of MapLibre.

So Metal support was started, but OpenGL was always going to be required, so the projected needed a multi-toolkit approach where it could swap in rendering SDKs as needed.  Thus the Drawable Architecture was designed and implemented.

As of early 2024, Metal was working on iOS and MacOS platforms.  OpenGL continued to work on all the rest.  Vulkan support for Windows and Android was added near the end of 2024.  

Metal was difficult as we implemented the new Drawable Architecture at the same time.  Vulkan was much less so and was implemented in much less time.  We can conclude the Drawable Architecture approach worked and we can rely on it going forward.

## Platforms & SDKs

MapLibre Native primarily supports two platforms: iOS and Android.  It also supports Qt, Windows, Linux and a number of others.  What that looks like is at least trying not to break those platforms when we change or add something.

Across those platforms we now support three rendering SDKs: OpenGL ES, Metal, and Vulkan.  It's likely that MapLibre will always support OpenGL going forward, even if the focus moves away on the primary platforms.

And so three or more platforms and at least 3 rendering SDKs.  

## Drawable Architecture

The original MapLibre rendering architecture was very much welded to OpenGL.  Each Layer type, such as wide lines or polygons, knew how to draw itself to OpenGL with the aid of a lot of C++ templated helper logic.  It worked fine, but was not a design that lent itself to more than one rendering SDK.

The Drawable Architecture was designed in 2022 as a very basic level of abstraction.  The idea of Drawables goes way back in 3D graphics and you'll find references to it in things like [OpenSceneGraph](https://github.com/openscenegraph/OpenSceneGraph/blob/master/include/osg/Drawable) and its [predecessors](https://en.wikipedia.org/wiki/OpenGL_Performer).

Figure: Drawable layer sitting on top of the SDKs

We come by this approach from the [WhirlyGlobe](https://mousebird-consulting-inc.github.io/WhirlyGlobe/) toolkit, where it's used extensively in combination with a threaded approach for loading data.  It didn't map perfectly to MapLibre and the seams between the old and new approach are evident in things like the Buckets, which we'll discuss later.

Nonetheless, the Drawable approach worked well enough to support 3 rendering SDKs and is the approach we'll use going forward.

### Drawables

If it's a Drawable Architecture, then what are Drawables?  They're the smallest unit that knows how to draw itself.  They contain all the bits and pieces necessary to draw a well defined chunk of geometry.  That might be all the roads in a tile, or it might be a whole bunch of symbols generated by the layout engine.

Drawables don't map one to one to tiles, necessarily.  Most drawables are the product of tiles and don't span multiple tiles, with the exception of the output of the layout engine.  One tile can, and probably will, produce multiple Drawables.  Drawables are build by the render layers, which we'll talk about individually later.

A nearly exhaustive list of what's in a Drawable is like so:
- Texture IDs for the textures to apply to this geometry.
- The render pass the drawable belongs to.  This might be something like 'translucent'
- Shader ID for the shader that will be executed for this drawable.
- Line width, which is really only germane to OpenGL and sometimes Vulkan.
- Color mode
- Cull mode, such as front facing, back facing, etc.
- Vertex Attributes.  This is vertex data like x,y,z or color, or a whole host of weirder stuff.
- Index Data.  This strings the vertices together into (usually) triangles.
- Origin of data.  All tile data has a local origin.
- Uniform Buffer Objects (UBOs).  These control how the geometry will look and where it will be placed.

Figure: Turn this into a figure

We've more recently added support for instancing too, but that kind of complicates things.  Let's look at some of those bits in more detail.

The core of this is the Vertex Attributes and the Uniform Buffer Objects (UBOs).  Put simply, Vertex Attributes tell the system what to draw and UBOs tell it how to draw those things.

Let's take a really simple example:  A red triangle with a repeating grass texture.  Well, more likely 10,000 of them.

The vertex attributes will contain the geometry for those triangles.  The vertices.  They'll also probably contain the color red, unless we override that and shove it somewhere else.  The texture ID for the repeating grass texture will be in the drawable as well.  The indices will string all those vertices into triangles.  Then there's the UBOs.

UBOs contain *everything else* which is details like the matrix to put the geometry into place or default attribute values.  The latter is things like color that cover the entire set of data.  You see, MapLibre can decide to pass one input valid in for everything in a Drawable by using the UBO or it can pass in individual values through the vertex attributes.

You might say it would be easier to just always pass everything in and we could simplify the whole system.  It turns out to have serious performance implications.  And so we sometimes get color (and other attributes) from the vertex and sometimes get them from the UBOs.  This complicates the design of the shaders, as well see later.

There are per Drawable UBOs and per Layer UBOs as well as at least one global UBO.  The global one contains information like the overall model matrix.  The per Layer ones contain data that corresponds to a given layer, of which there can be hundreds in a Style.  The per Drawable ones pertain just to this Drawable.

Each rendering SDKs subclasses the Drawable for its own purposes.  Thus there is an Metal Drawable, OpenGL Drawable, and Vulkan Drawable at present.

One of MapLibre's big features is that it changes visually as you zoom in or out.  It can do a lot more than that and it does it through attribute based expressions.  The most obvious of these is the change in width of a road as you zoom in and out.

That width change is expressed through the UBO for a line and it can update every frame.  The way we do that update is through the Tweakers.

### Buffers

Most people think 3D rendering is about linear algebra and 3D objects.  I like to say 3D rendering is mostly memory management.  When communicating with a Graphics Processing Unit we're slinging a lot of memory to it, and sometimes, memory back from it, though you want to keep that to a minimum.

There are various ways to communicate that memory and as graphics SDKs have gotten more sophisticated they've gotten more explict about what's going on.  Where in OpenGL we had named and typed uniforms, in Metal and Vulkan we tend to have blocks of memory that the reader and writer happen to know the structure of.  In other words, we mostly have buffers.

We've got buffers for our vertex array objects.  We've also got buffers for our uniforms.  Sometimes we have a small value used to tweak things, a bit like the old OpenGL uniforms, but those are the exception now.  Vulkan has Push Constants and Metal has something similar without the cool name.  But for the most part, we create buffers, we delete buffers, we update the contents of buffers, and we orchestrate all of that to not step on our own feet.

Buffers are just chunks of memory, allocated either on the CPU or GPU side (the same for mobile devices) and owned by one or the other.  We put our vertex data or uniform data in the and wire them up to the right location in our encoding phase.  Then we try not to destroy them or overwrite them while they're in use.

MapLibre Native was written at a time before you could reliably do things with the GPU on multiple threads.  So Buffers are always created, destroyed, and uploaded on the main (or render) thread.

The process typically goes like this for building a vertex array, starting on a background thread:
- Allocate a block of memory
- Add vertex data to that memory
- Hand the memory over to the main thread
- Create a buffer, using whatever the rendering SDK provides to track it
- Upload the contents of the memory into that buffer
- Track the buffer by rendering SDK ID to delete it later

For Uniform Buffer Objects, discussed later, the process is all on the main (or render) thread:
- Allocate memory for a UBO
- Fill in values, usually with a struct or something similar
- Allocate a buffer using the rendering SDK
- Copy the struct to the buffer
- Track the buffer by rendering SDK ID to delete it later

Drawables are then, for the most part, just a collection of buffers, both for vertex arrays and for uniforms.  All the rest is state information that doesn't fit anywhere else.

### Drawable Tweakers

Things change as the user moves around.  A whole lot of things change.  Zoom level changes are the most obvious, with color interpolation or roads changing width, but there's a lot more of this going on beneath the surface.

MapLibre was designed with a sort of pre-OpenGL ES 2.0 worldview.  They assumed they could put their hands on any part of the geometry or instructions going to the rendering SDK at any time.  This made sense as they were re-encoding everything every frame.

As maps got denser, GPUs got smarter, and frame rates went up past 60hz this approach can get annoying.  CPU capacity has gone up in conjuction, so it's worked out reasonably well, but it still has problems and it wastes a lot of power doing very little.  There's also issues with having multiple frames in flight, as modern SDKs prefer to do, and stepping on a frame in progress with changes for a new one.

In the more modern approach with rendering SDKs like Metal and Vulkan, or even OpenGL ES 3+ we don't really want to re-encode everything each frame.  We want to bundle it all up once and only modify it a little bit.  That's what Drawable Tweakers do.  They modify the Drawables and UBOs that have already been built based on changes to the view, what's been loaded or unloaded or even just input from a developer.

Drawable Tweakers mostly modify the UBOs.  If the zoom level changes, a Line Tweaker will calculate a new line width and update the Line UBO.  That change will then propagate to the shader on the next frame and the line will be drawn with the new width.

The Tweakers can also modify geometry in the Drawables themselves and this can get dicey.  The raster layers do this as they load new data and try to fill in the holes with what's not loaded.

The intent behind Tweakers is to abstract the render data from the actions taken on it.  In the previous architecture Render Layers just reach into the geometry and attributes to mess with them directly.  That has largely been abstracted out to Tweakers, but there are unfortuneately a few places where the Render Layers will mess with geometry.

One last goals for Tweakers was to carve out their functionality to eventually eliminate them.  It has to do with data driven attributes and where they are updated.

### Uniforms & Uniform Buffer Objects

Vertex Attributes, along with indices for triangles, roughly tell the GPU what to draw.  This geometry, at this location, covering this part of the screen, more or less.  Uniforms tell the GPU how to draw it.

At the end of the day it's all just memory buffers, mostly, but how those are used can be very different.  Where a rendering SDK iterates over the triangles and vertices you give it, it typically just takes just a handful of uniform values to cover that whole group of triangles or lines or what have you.

You can get much more complex these days where even that doesn't describe what modern GPUs can do.  They can even build their own instruction sets, but we're not using anything at that level in MapLibre.  We mostly stick to the vertex buffer + triangle indices + uniforms paradigm.

So uniforms are descriptions, instructions on how to draw the geometry we give the GPU through vertex attributes and indices.  They might be as simple as a color, or an opacity, or a bit more complex like a matrix to place our geometry.

Older versions of OpenGL named these uniforms and called them out as individual objects.  For instance, color might be an RGBA value of 4 floats stored in 8 bits each.  Later versions of that SDK and more modern SDKs expect us to pass in struct-like objects in blocks of memory.  Obviously we can't get too fancy here, so they tend to be laid out in the most mundane way possible with padding to keep things from getting too tricky.

Thus was born the Uniform Buffer Object.  It's a block of memory (it's always blocks of memory) named as a single unit that can contain whatever you like.  Or whatever you like within the constraints defined by the SDKs.  In general these contain matrices, colors, floats, ints, that sort of thing.  If you want to do an array, it's typically a fixed length array.  There are ways.... but we try to keep it simple.

UBOs are our way of telling the shaders what to do with our geometry and we'll use the term Uniform or UBO interchangeably.

Figure: Example of one of our UBOs

### Data Driven Attributes & the GPU
Think back on the line width driven by zoom level.  That's an easy example where we change one value within the line UBO and lines get rendered differently.

Figure: Style Sheet Example

Some attributes go in the UBO and can be changed once.  Other attributes go in the vertex attributes and have all be changed.  If you have color based on zoom level and you have to combine it with colors coming from where else you may end up recalculating all the vertex colors every frame.  That's the cost of flexibility style expressions give you.

We have tried to isolate much of that logic.  Thus moving it into Tweakers where it can be treated more generically and then, eventually, moved to the GPU itself.

Ideally we'd like to run all the per frame data driven logic on the GPU.  That would elminate memory traffic from the CPU to the GPU and greatly simplify the encoding process.  We could encode once, for the SDKs that allow it, and make any per frame changes on the GPU side.

We've only partially succeeded in that goal.  The Tweaker are isolated to their subsystem, mostly, but we've only experimented with running them on the GPU.  There are, as yet, other threads to disentangle before that becomes effective.

## Layers

If you're at all familiar with style sheets you know about layers.  There can be hundreds of them in a given style and they can all do slightly different things.  Each of those Layers has a Layer Type, like Fill or Line or Symbol and those have corresponding objects in MapLibre Native.

When we refer to a Layer henceforth, we really mean a Layer Type.  There's one for Fill, Background, Circle, Raster and so forth.  Those are Layer Types, now Layers and they live closer to the style sheet.  They do the parsing, setup, and teardown and so on.  

As the system is parsing the style sheet it's emitting new Layers.  This means the layers appear individually and can start rendering separately from each other.  Users don't tend to notice this, but it's a real effect.

Each of the Layer types has a Factory and could theoretically be overriden by a developer, but that's not terribly practical.  These things have such arcane dependencies it's hard to replace them.

Figure: Layers and Render Layers

Each Layer has an associated Render Layer and there we'll spend more time.

## Render Layer

For each Layer there is a Render Layer and its job is to, logically enough, Render that Layer type.  For each layer in the style you'll have one instance of that layer and one corresponding instance of the render layer.  This can be a problem with a lot of really small layers, incidentally.  By breaking layer geometry out by type we can't easily combine it back together for rendering.

### Geometry Construction

A Layer's geometry will be typically parsed out of a vector tile on a background thread.  The Layer will use a Bucket to construct GPU friendly memory buffers and consolidate some geometry after tessellation.

There is a corresponding Bucket type for each of the major layer types (Fill, Line, etc).  The rules of how these work in combination with the style expressions get a bit complex.  This is an area we weren't able to do much with and we've largely left the buckets intact.

Buckets are responsible for building the arrays typically passed to the GPU, or close enough to do direct translation.  Some layers are fairly direct, like the Fill after tessellation.  Others require some construction, such as the circle or line layer.  Those build geometry around points or centerlines and can get fairly complex, particularly the geometry for lines.

Buckets have to take into account the rules for expression evaluation and this makes them rather complex and opaque, at least to us.  When constructing, say, the color vertex attribute for a line you might get several different outputs:
- A single constant for the color to be applied to a whole group of lines
- A color that can change based on zoom, for example, but for the whole group of lines together
- A vertex array that contains color for each line vertex
- A vertex array just as before, but can change based on zoom

There are even cases where one of those can turn into another after a layer is loaded.  In those cases we might have to go back to the Builder to make us a new geometry.  Style expression complicates everything.

But in general, we feed raw vector geometry into a Bucket and get the memory buffers needed to drive the GPU.  Or something very close to that which can be translated fairly easily.

### Render Layer Operations

Drawables are not built until the Render Layer runs.  The Buckets will build the memory for the buffers used by the Drawables, but it's up to the Render Layer to actually build the Drawables around those buffers.  It's also up to the Render Layer to destroy the Drawables when done with them.

The order of operations for every frame in a Render Layer looks something like this:
- Figure out which tiles to load based on which tiles are in view.  Each Layer does this separately, which is a bit odd, but hard to disentangle.  Also ask layer data for a given tile to start parsing.
- Process new Drawables out the Buckets that have finished processing their tile data.  Also create Drawable Tweakers to go with them.  Then upload the data for those Drawables.
- Update existing Drawables as needed.  Some expressions result in very complex changes to buffers, and thus drawables, and that work has to be done by the whole Render Layer rather than just the Tweaker.
- Tear down old Drawables that are no longer needed.  Also clean up Bucket data if it's not shared.

There's some tension between the architecture we have and the one we want.  Ideally we'd like all the Drawable changes moved into the Tweakers, but that's not quite possible.  Ideally we'd like tile loading decisions to be made once, but there are some corner cases that make that difficult.  We move closer to the ideal, but these are the tradeoffs we make to avoid breaking the system.

### Style Expressions

One of the things a Layer will pass to its Render Layer is Style Expressions as they're tied to specific geometry.  By this point style expressions have been boiled down to Unevaluated and Evaluated.  

An Evaluated style expression works out to a constant and will typically be represented by a value in a UBO for a particular drawable or Layer.  
Question: Does a Zoom based expression show up as Evaluated or Unevaluated?

Figure: Example of unevaluated expression based on zoom level.

An unevalated expression has to potentially be run at each frame.  There is some logic to test if a value has changed and thus avoid re-running the style expression, but if the value, such as a zoom level is changing, then it will have to be run.

The Render Layer runs these Unevaluated style expressions, turning them in Evaluated style expressions that can then be written out into a UBO, or in more complex cases, an updated Vertex Array.  The latter might happen if colors were being calculated per vertex with a style expression.

Thus a style expression can force a recalculation of something as small as one data value in a UBO or something as big as a whole vertex array.  For the most part we've moved the former into the Tweakers with the latter continuing to exist in the Render Layer.

### Fill Render Layer

The Fill Render Layer handles drawing filled polygons, really triangles.  The triangles are tessellated in the Geometry Worker on a background thread then run through the Fill Bucket to construct the triangle indices and buffers needed by the GPU.  The Fill Render Layer takes ownership of these, wrapping Drawables around them, building the UBOs to instruct the selected shader to draw them.

Figure: Picture of Filled Polygons
Figure: Style sheet Fill Layer example

This layer uses one type of Drawable for all its various types, but connects different shaders and different UBOs to each depending on the type.
- Fill: The basic one, this takes a group of triangles and colors and draws those to the screen.
- Fill Pattern: Like the basic one, but with a texture to be applied.
- Outline: Style sheets allow Fill to have outlines.  If present, this is the Outline.
- Outline Pattern: An outline can also have a repeating texture.

Each of those types has an associated Shader and unique UBO. They'll also share a UBO for the Fill Layer and be given the global UBO for things like model matrix.

#### Fill Outlines

Fill outlines are a very different case than normal Fill.  They're lines, but lines are tricky now.

Figure: Picture of Filled Polygons with Outline
Figure: Style sheet Fill Outline example

OpenGL and some variants of Vulkan provide a very simple line implementation.  You specify a width and a color and off it goes.  Metal does not do that, requiring you to implement your own line.

We fixed this in the Drawable Architecture by calling on some of the Line Layer logic to build the Line geometry for us.  Thus in OpenGL it still does it the simple way and for Metal and Vulkan it uses our own widened line logic.

Question: How did Outline Patter work before?

#### Fill Layer UBOs

TODO: Figures for each of the UBOs in use by the Fill Layer and an explanation of what they do

#### Fill Layer Tweaker

This Tweaker runs through all of the drawables for a given Fill Render Layer and updates the general and then specific parameters in the UBOs.

General parameters include the matrices for each tile.  You might assume the tile matrices could be set once, but there are some very twiddly inputs to the tile matrix that can change based on location and expressions, thus we may recalculate the tile matrix more often.

The specific parameters are broken into groups based one what kind of Fill a given drawable belongs to.  This includes Fill, FillOutline, FillPattern, and FillOutlinePattern.  Each of those has its own UBO with its own parameters and many of those can be adjusted based on real time style expressions.

The Tweakers do most of this work, but some larger changes have to be run by the Fill Render Layer itself.

### Background Render Layer

The Background Layer is what draws everything that's not drawn by something else.  For most maps it's drawing the land, rather than the ocean.  The ocean is typically defined by polygons and the land is typically not.  That's not a requirement of the data, it's just convention and you could set it up separately.

The Background Layer draws first and behind everything else.  If it's very simple in the style, we'll just set a Clear Color and the rendering SDK will draw that.  If it's more complex, then the Background Layer does go through a simplified version of the same steps as everything else.
- Determine which tiles are loaded.
- Create geometry for each of those tiles and associated Drawables.
- Pick the proper Shader and UBOs for those Drawables.

There are shaders and UBOs available for plain color as well as pattern shaders.  Since these can be modified by style expressions, the Background Render Layer has to run the expression logic and update UBOs accordingly.

Question: If there's no tweaker, how is it updating the color based on zoom level?

### Circle Render Layer

Circles are one of the simpler layers.  They take points as input and draw a circle at each point.  That circle can have an inside and an outline, but no texture.

Figure: Circle Example
Figure: Style Sheet Circle Example

The Circle Render Layer goes through the same steps as other render layers, but it only has one Shader and one UBO.

Question: Do we do the circle outlines?

Expressions can control the color, opacity and blur as well as translation, pitch and stroke releated properties.  It's these that can cause a recalculation of UBO values during the update.

Question: How do we update these without a tweaker?

#### Circle UBOs

TODO: List the Circle UBOs and contents

### Symbol Render Layer

The Symbol Render Layer handles both icon symbols and glyphs and we'll refer to them together as symbols in this section. 

Figure: Symbol Example
Figure: Style Sheet Symbol Example 

The way symbol and glyph images are represented is as textures.  MapLibre Native typically pulls these from sprite sheets which are provided as part of the style spec.  Sprite sheets are just larger conglomerations of textures with indexing information.

Question: Do we ever pull textures from the native glyph engine?

When a Symbol Layer loads data it uses a Builder to construct the necessary geometry on a background thread, much as the Fill Layer does.  It sets up a variety of vertex attributes controlling color, opacity and location.  Those can be modified by the Layout Pass, discussed next.

In addition to the geometry, we need the textures to render text and symbols.  The way MapLibre handles these is a little unusual.  It will form a single texture atlas (or sprite sheet if you like) for a given tile.  Thus we have one texture atlas per tile that's formed on a background thread, uploaded on the main thread and destroyed when the tile is deleted.

There are some advantages to this approach.  You can assume a tile's textures will never get larger than a certain fixed amount and so you'll never have to reference more than one texture per tile as you might in a global shared texture atlas.  The disadvantage is the redundant memory use, of course, but this will only really be a problem if you start using large textures and vector maps don't at present.

As with the other layers, the symbol layer renders tile by tile, but not all of those symbols may be turned on or in the same physical place they started at.  Turning those on and off and nudging them around is the responsibility of the layout pass.

#### Layout Pass

Symbols and Glyphs can collide as the user zooms out or even between tiles.  That's not usually what's desired so we have a Layout Pass to work out those collisions, decide what's going to appear and how it may be nudged to accomplish that.

Figure: Layout Example

Symbols (and Glyphs) are per tile, but the logic to deconflict them runs once every frame, feeding its results back into the Symbol Render Layer.  Thus the geometry for symbols is defined ahead of time and then enabled, disabled, and nudged by both expressions and the Layout Pass.

Question: Do we really disable by setting opacity to zero?

The Layout Pass works all this out and then passes instructions back to the Symbol Layer to do the nudging.  This can be changes in geometry that reflect layout preferences (e.g. left, right, center) or it can instructions to fade out a given symbol because it conflicts with something else.  These are fed into the vertex attributes directly via some combination of Symbol Render Layer action and Tweaker.

#### Key Sorting

An option in the style spec for symbols is to sort your features back to front by key.  Each feature will then have a sort value and MapLibre will generate one segment per sort key, grouping all the features with that key together.

If you're not using key sorting, you get as few segments as the Symbol Layer can manage.  This tends to be more efficient so use Key Sorting sparingly.  

When Drawables are formed they'll be per segment.  Segments are just strings of symbols we can put together.  Without sorting one tile will tend to have a small number of segments, or even just one.

TODO: Run this by Tim and get a better explanation

#### Symbol UBOs

TODO: List the symbol UBOs and their uses.

#### Drawable Atlases Tweaker

This tweaker is started by the Symbol Layer to update references to the Texture Atlas from the Drawables.  Textures can change one a frame by frame basis and this Tweaker will make those changes to the Drawables created by the Symbol Layer.

Question: When do textures change per frame?
Question: Not totally satisifed by this explanation of what the Tweaker does.

#### Symbol Layer Tweaker

This Tweaker operates for a single Symbol Layer at the level of the common and combined UBOs.  It also iterates over the Drawables and assigns UBOs to them to update information.

TODO: Get more of an explantion from Tim.

### Line Render Layer

Lines are important in vector maps.  We use them for roads, we use two of them to make a road casing, we outline things with them, they form very narrow wires and we'll drap textures on them to make things like railroads.  They do a lot of work for a map and are more complex than they might seen.

Even with OpenGL MapLibre couldn't just fall back to the driver line implementation.  They needed really wide lines, beyond what most drivers would provide, they needed to run textures down the middle, and they wanted fancier turns.  Thus a custom Line implementation.

The Line Layer will build geometry for the Line Render layer by widening the lines and building vertices and triangles to represent them.  The Line Bucket does this working, producing vertex attributes that can then be wrapped by a Drawable in the Line Render Layer.

Lines are expandable with width defined at run time.  One of the most common style expressions is a zoom based width calculation.  Thus the geometry and the shader to facilitate that is a little complex.  There are four variants with four different shaders.

#### Simple Line

This one is pretty simple and just implements a width controlled single color line.  It's mostlyused for roads.

#### Gradient Line

This variant uses texture coordinates to drive a gradient texture along the line, but is otherwise the same.

#### SDF Line

SDF refers to Signed Distance Field and this is the Line Glyph renderer.  If you put text along a line it's being rendered here.
Question: Is this right?

#### Pattern Line

If a line has a pattern, like a railroad for example, it's a line with a repeating texture and implemented by this variant of the Line.

#### New Wide Lines

We talk about this more in the Misc section, but there is a newer implementation of Wide Vectors.  It's based on a more GPU forward approach, but does not implement all the features necessary to replace the Line Layer.

### Raster Render Layer

This Layer handles both the image per tile case for a traditional raster map and the case of a single image dropped at a specific location.

Figure: Picture of Raster in action

Most of the interesting logic in the Raster Layer is associated with loading.  When a tile comes in to view, MapLibre needs to load it.  When that happens, the image is handed back to the Raster Layer and our part starts.

The image itself is converted to a texture and uploaded to the Rendering SDK.  Unfortunately this happens on the main (render) thread.

The way it does work is the Raster Layer sets up an ImageLayer Group so it can apply a UBO to all the Drawables beneath it.  We get one Drawable per image tile that represents a rectangle and sub-rectangles to mask loading.

One of the more complicated bits is the mask used to update vertex arrays on the fly.  When the toolkit is loading new higher resolution tiles, it will turn off the rectangles for a high res tile that's loaded while it's still loading the others.

Image a Level 4 tile.  The system has loaded two Level 5 tiles and is waiting for two more.  Rather than wait for them all, it will turn off two pieces of the Level 4 tile and draw the Level 5 tiles.  This is what masks do and they're a bit complicated in implementation.

As an aside, before interacting with MapLibre Native I was curious how they addressed problems like texture proliferation, running multiple Raster Layers at once, batching uploads, and multiple Raster Layers running at different resolutions with different loading parameters.  The answer it turns out, is they don't.  MapLibre is very much a vector map toolkit and there's a lot of room for improvement of the Raster Layer.

### Heatmap Render Layer

Heatmaps are interesting in that they're a layer derived from data and have to be rendered in a two step process.  This was a really great idea, but it's implemented in a "one off" way that doesn't let us reuse any of the concepts elsewhere.

Figure: Heatmap Example

The way it does work is the Render Layer sets up one Render Target for the whole screen.  Those are described below, but the short version is we draw to a Render Target.  That target is the size of the visible screen.

For each tile in the Heatmap the Layer adds a drawable to then Render Target which uses the heatmap shader.  That shader draws objects into Render Target as values, rather than visuals.  

From that Render Target the Layer uses the heatmap__texture shader to do a bit of filtering and draw the results to the screen in a visible way.

### Hillshade Render Layer

The Hillshade layer draws elevation data in tiled form to the screen.

Figure: Hillshade Example

It mimics some of the same Raster Layer logic for loading and masking.  Really, this whole part of MapLibre could use a consolidation and rewrite.  There are some great concepts in here that aren't reused well.

What the Hillshade data does do is manipula DEM data.  These are tiles of Digital Elevation Models and each pixel is an elevation rather than a color.  To draw these properly you need a little border around each DEM and the Layer handles backfilling those as they load.

The Layer sets up one texture per tile and uses a Render Target and a Drawable to render the DEM data for that tile directly to an output textures.  This uses a custom shader that turned elevation into color (and shading).

Hillshade Buckets build the actual geometry and are marked dirty when neighboring buckets load to deal with the edge problems and to rerender to individual target textures.

Those textures are already on the GPU side and so don't have to be uploaded.  They are tied in with a Drawable per tile and all drawn together into the visible target using a fairly generic shader.

## Render Targets

Render Targets as a general concept go something like this.  Developers tend to think of rendering SDKs as drawing to the screen, but they don't have to.  Sometimes they draw to memory and that memory is used for something else.  It may be part of a multi-stage pipeline that eventually leads to the screen, or it may not.

A Render Target is where we're pointing MapLibre and that is generally the screen, but sometimes it's not.  As in the Heatmap example, we're drawing in two stages.  The first is drawing data and the second is drawing that data to the screen.  A Render Target for each tile is what we're drawing to in that first step.

In MapLibre Render Targets are only used in a few places and aren't terribly flexible.  It would be useful to open that up and let the developers establish their own pipelines, but that's not possible at the moment.

As to how they work, you can create a low level Render Target with one or more textures for color, depth, and possibly stencil.  We also associate a Layer Group with that render target and we can put Drawables in there to be drawn every frame.

## Texture Atlas

The idea of a texture atlas is pretty similar to a sprite sheet.  You take a whole bunch of images and you glue them together into one big image.  Then instead of referencing one image for a symbol, say, you create a handle which gives you an (x,y) offset and a size.

As to why you do that, it depends.  For sprite sheets it's more efficient to move one big image around than a bunch of small images.  For textures in a rendering SDK it's a matter of memory access and efficiency.  It's faster to have one big texture than a lot of little ones.  It's also easier to refence one texture than it is 10 or 20 in a shader.

The Atlases themselves are built in the tile loading phase.  When parsing a tile and building individual layers, MapLibre will construct a unique Atlas for a given purpose, copying out or generating images as need, packing them together and presenting a single image for that purpose.

The main (or render) thread will take that Atlas image and upload it during the Upload phase of a given Render Layer.  When the tile is unloaded, the Atlas will be destroyed.

What's good about this approach is we can know that there's only one texture in a given Render Layer being used for an Atlas.  This simplified geometry grouping logic and the shaders.

Question: Do the render layers share Altases within a tile?

What's bad about this approach is the upper limit on texture size and the copying.  If a given render layer exceeds the amount of space used for a texture you're just out of luck.  As for copying, multiple tiles will have the same images in their Atlases without sharing.

The way vector maps are currently structure, neither issue comes up.  The symbols tend to be small and cartographers don't use many of them.  This could become a problem if geospecific textures became popular.

Texture Atlas is the base concept, but the actual implementations are for Glyph, Icon, and Pattern.  Each of those is discussed below.

### Glyph Atlas

Logically enough, the Glyph Atlas contains all the glyphs used in a tile.  The glyphs are a little funny, as they're not simple images for characters.  They're [signed distance fields](https://en.wikipedia.org/wiki/Signed_distance_function).  This has a number of odd consequences.

All the various platforms MapLibre Native runs on can generate glyphs for us, but we use none of it.  Instead we read the glyphs from sprite sheets constructed as signed distance fields.  There are a lot of downsides to that, but it was built into the architecture of the web version and they ported that over to mobile at the time.

For the moment, this is what MapLibre Native does and so we support these.

The Glyph Atlas is built per tile in the Geometry Worker and then uploaded on the main (render) thread after the whole tile is handed over to the main (render) thread.

Like all the Atlases there's no sharing between tiles and each tile creates its own.

### Icon Atlas

Icons Atlases are much like Glyph Atlases but for icons.  The icons themselves come out of sprite sheets, begging the question why we don't just leave them there, but this is the current architecture.

Icon Atlases are built per tile and also uploaded on the main (render) thread.

There ever never very many of these, so upload doesn't appear to take much time.

### Pattern Atlas

This Atlas is a bit different from the others in that it contains the generated patterns used for things like dots and dashes.

Figure: Example of dashes

The Pattern Atlas is global, with subregion uploads and deletions happening on the main thread as requested by background threads when they merge back into the main (render) thread.

In theory the Pattern Atlas can run out of room, but practically speaking cartographers use patterns sparingly in their vector maps.

## Custom Layers

Custom Layers are, colloquially speaking, plugins.  It used to be that plugins had to be loaded from storage at run-time, but now we refer to anything that can be inserted into an existing toolkit without recompiling as a plugin.  I honestly can't think of a better word, so Custom Layers are plugins.

Their purpose is to add very specific rendering to the map, to draw something that MapLibre can't do.  At our company, Wet Dog Weather, we use Custom Layers to draw weather overlays.  There's a whole complicated pipeline behind that which just intersects with MapLibre at the Custom Layer level.

A really obvious use for the Custom Layer is a user location overlay.  The puck, it's called.  It needs to be updated every frame and you can do that with a native mobile implementation (e.g. UIKit), but it's going to work better and faster if you render it locally.

Early in our development of Metal we had wanted to greatly expand the Custom Layer implementation.  Both to allow outside developers to add new functionality, possibly replace existing functionality and maybe even implement MapLibre Layers in shared functionality we could all use.  It didn't work out that way.

There are two kinds of custom layers, one platform specific and one not.

### Custom Layer: Platform Specific

The platform specific layers target OpenGL or Metal directly.  If you want to draw with those rendering SDKs you need to pick the right one and implement your specific rendering in the callback given.

In concept this is simple enough.  For Metal you're getting Command Buffer.  For OpenGL you get a context.  MapLibre also provides information about the current model/view/projection matrix and enough context to make it all work.  You can do what you like from there.

What we don't provide is access to all the various internals to make your full Layers.  You really have to do all the work yourself, but this still works quite well if the developer knows what they're doing.

The best way to approach Platform Specific Custom Layers is to look at the examples which can be found in the test apps.

### Custom Drawable Layer: Platform Agnostic

When everything was OpenGL, the Custom Layer for OpenGL was all you needed.  Now that we support Vulkan, Metal, and OpenGL developers have to implement in all three if they want full support for their custom functionality.  The Custom Drawable Layer was meant to fix that and is partially implemented.

The idea here was that developers could insert their own Custom Layer at the C++ level that would run on any platform.  That Custom Layer would build Drawable geometry and pass it off to the system to render and manage.  In this way developers could add real time layers that work cross platform and only things like the user puck once.

The implementation for Custom Drawable Layers was never completed, but it can support simple symbols and polylines.  We hope to build it out a bit more in the future.

## Render Orchestrator

Up to this point we've been talking about layers as individual units of representation and rendering.  That begs the question: What coordinates the Render Layers?  That's what the Render Orchestrator is.

Figure: Draw the Orchestrator sequence

Render Layers don't run linearly, they are called up to do individual units of work in sequence.  It's the Render Orchestrator that decides which layers run and in what order.  That order is derived from the style sheet or individual layers inserted into the style sheet programmatically.

The first thing the Orchestrator does is create the Render Tree.  
TODO: Get a better explanation of what this does

Question: Who kicks off the loads for a tile with a layer?  Does the render layer or something else?

Then the Orchestrator has the layers upload their new data.  It's somewhat up to the Render Layers as to what they'll upload.  They're often communicating with other threads to wait for data to load or be created or what have you.  But when the data is there and the memory buffers created, each Render Layer will upload its data in place.

This communication is actually a bit of a problem because it involves mutexes and occasional delays.  In general you don't want to be doing this on the main (or render) thread.  You want the encoding to just 'go'.

Uploading on the main (render) thread is also a bit of a waste.  This is a holdover from early versions of OpenGL and many of the buffers could be uploaded on background threads.  This would then simplify the loading state machines in the Render Layer.

Good changes for the future, but this is how it works now.  The Upload stage is kicked off by the Orchestrator and each Render Layer with data to uploads does it right then, one after the other.

Next up is the actual rendering and that will happen in multiple passes.  First is opaque and then translucent.
TODO: Add an explanation of the difference and who uses translucent.

Question: Does the orchestrator upload images itself or is that in the layers?

## Shader Subsystem

Geometry is what we're rendering, UBOs are how we're rendering it and shaders are the machine for doing the rendering.

OpenGL ES 1 had a fixed function pipeline, which means no shaders.  With 2 we got vertex and fragment shaders and with later versions and newer toolkits we get other variants.

We're sticking to fairly mundane shaders along the vertex/fragment model.  That is, we have one shader for vertices and another shader for fragments and they talk to each other vertex to fragment.

Our vertex shader for a given set of geometry will be be called once for every vertex.  If we're using instances, it's once per vertex per instance, but we don't use instances very often.

The main point of a vertex shader is to transform imput geometry from our own tile space to a normalized viewing space that translates directly to the screen.  When using Z buffer, which we mostly are, geometry will appear back to front and anything outside our viewing frustum disappears.

So we get some sort of tile coordinate in and we turn that into true 3D geometry for the rendering SDK to chew on.  We also output other values like color or opacity or a host of other modifiers the fragment shader will use to generate pixels.

Our fragment shader for a given type of geometry takes a single point as input.  It's called once per point that will appear on the screen from a given set of geometry.  Its output is almost always color and depth.

Vertex information flows from the vertex shader to the fragment shader and it's here that things get tricky.  GPUs are very sensitive to the amount of data flowing from the vertex to the fragment shaders.  That data needs to be interpolated and fragment shaders are tiny little programs meant to run as fast as possible.

If you pass everything that can change from vertex to fragment shader, it's much slower than just passing what is changing.  We'll discuss that in more detail when we talk about permutations.

We associate vertex and fragment shader together into one bundle of shader, but that's an artifact of OpenGL.  It's not as necessary for Vulkan or Metal.

### Vertex Array Coding

32 bit floating point isn't accurate enough to get really big or really small.  Trust us on this one, things start vibrating if you do it wrong.  As a result, MapLibre keeps its vertices represented in a local tile coordiate system.  These have to be translated, scaled, and even rotated into place within the shader itself.

Vector Tile vertex data is offset from the origin of the tile itself and compressed into as few bits as possible.  They really leaned into this throughout the system and vertex arrays are code in much the same way.

What this means is that looking at a vertex array containing geometry in a debugger won't tell you much.  It's encoded into a bits in a non-obvious way and has to be decoded into floats as the first step in any shader.

Figure: Coded vertex array example

Once decoded, those floats are offset from the origin of the tile as you'd expect.  There's a matrix provided in one of the UBOs to properly place those in model space and then another matrix at the global level to move from model into model/view/projection space.  Thus we can get from local coordinates, coded in a specific way, to vertices in the normalized projected space, which the rendering SDK can chew on.

This does introduce a problem with data from sources other than vector tiles.  Without a general purpose way of encoding larger values we have to turn everything into a tile, at least in the abstract.  That limits the scale of geometry so you can have something large that has very little accuracy or something very precise, but small.

### Shader Registry

When MapLibre Native starts up it register all the various shaders in the Shader Registry.  There's a template form to this that also registers a fair bit of Reflection data, discussed below.  That looks something like this.

Figure: Example of Shader Registration logic

When a Render Layer needs a particular shader it will ask for it by name.  The metadata for that shader includes which vertex attributes are wired up to which inputs and which UBOs go in which slot.  A bit more on that later.

Shaders can only be added at startup and there is no flexible mechanism to add shaders from outside the toolkit.  This would be useful for replacing existing shaders and facilitating development of toolkit internals without recompiling the whole thing.  We didn't quite manage this level of flexibility but might revisit it.

Shaders are compiled at runtime in MapLibre Native.  There are several reasons for this, including:
- Shader source is smaller than compiled shader binaries for Metal, certainly, and other platforms.
- Not all rendering SDKs offer compiled shaders so we'd need to include the source code.
- We actually use a number of shader permutations based on what the style sheet expressions require.

### Shader Permutations

We package the shader source code and compile variants at runtime as needed.  Depending on what features are used in a style sheet we may need to activate different parts of a shader for a given Layer.  It all comes down to the expressions.

Let's take the standard Fill Layer as an example with two different variants:
- Fill Color is a constant, defined as one non-changing value in the style sheet.
- Fill Color is a combination of per feature values and zoom level using an expression in the style sheet.

In the first case we can stick the color in a UBO for the Drawable and read it out in the fragment shader.  Vertex shader doesn't need to pass it on or have the GPU interpolate it between vertices.  All that logic can be turned off.  This is one variant of the Fill Shader.

In the second case we need to calculate a color value per vertex and have the vertex shader pass it on the GPU which interpolates it per pixel.  There's logic to pass that from the vertex to the fragment shader and associated member bandwidth and calculation cost.  The fragment shader will read that interpolated value from the vertex shader and apply it to the pixel.  This is another variant of the Fill Shader.

Thus we have two variants of a relatively simple Fill Shader, one more expensive than the other.  There are a handful of other values that can vary and would need to use that path other than color.  This can result in a small combinatoric number of shader variants which the toolkit will dutifully compile on request.

In practice style sheets tend to trigger only one or two of these variant compiles and the numbers don't tend to grow large.

### Shader Reflection

Most rendering SDKs will offer some level of reflection.  OpenGL, for example, let's you query the names of all the uniforms and UBOs, as well as attribute arrays and so forth.  Metal and Vulkan can get even more complex.  In general, we don't use it.

Reflection is great, but it's expensive and the bulk of our rendering SDK interaction is on the main (or render) thread.  Thus we don't want that expense at random points when we need a new shader variant.  We were faced with either trying to do this on another thread or coming up with a pre-defined way.  We went with the latter.

Each shader in MapLibre is registered with a list of array attributes, UBOs and indices it supports.  Those are wired up with constants in the Render Layers.  A given vertex attribute array is always at a given slot and UBOs are at well defined locations.

Any shared UBOs, like the global UBO that contains the overall MVP matrix, is tied in at a specific location in each shader.

Thus we traded speed for flexibility and made adding new shaders more complex.  It does work quite well, though.

### Shader Example: Fill

TODO: Pull one of the Fill shaders apart.

### Differences in Shaders between Rendering SDKs

TODO: Pull in Vulkan and Metal Fill Shader

## Miscellaneous

Here are a few topics that didn't fit in well anywhere else.  MapLibre Native has a lot of these cul-de-sacs and we'll explain a few.

### Wrapping the Whole World

When the user zooms all the way out they'll see multiple views of the earth repeated.

Figure: Picture

There are a number of ways to achieve this, but MapLibre uses just one, though it's a little weird.

Render Layers are responsible for managing their own tiles.  When the world needs to be drawn multiple times, they make up multiple tile IDs to do that.  Presumably just for tile 0,0,0.

The Render Layer will go so far as to create multiple Drawables for the multiple sets of geometry you see, those those Drawables will shader the same buffers.  They will have different UBOs, however.

One of the Drawable UBOs has a tile matrix in it and the extra tile ID will generate a different matrix, thus placing the new geometry away from the original tile.
Question: Which Drawable UBO.  Pick an example.

That's how we see the geometry repeated when zoomed out.  The system makes up new tiles IDs and makes a lightweight copy of the Drawables.  Thus we're drawing things more than once.

### Tile Borders and Tile Symbols for Debugging

Sometimes it's useful to see the tile borders, particularly when trying to debug something in the vector tiles or the style sheet or interaction between the two.  There is an option for that and it's handled explicitly in the renderer.

There is a Debug Layer Group we use for this that can render tile borders, tile text, or tile timestamps.  There's an explicit Debug Shader attached to these as well and the renderer will build Drawables corresponding to these.  They don't live in a separate layer, though.
Question: What are tile timestamps?

Symbol Debugging is handled with collision circles and boxes generated by the Symbol Layer.  These are computed by the Symbol Bucket, if turned on, and the Symbol Render Layer will build Drawables for them.

### Geometry Annotations

MapLibre Native started out as a vector tile rendering engine.  It doesn't just render vector tiles, though, and other sources were added later.
- Platform Native Annotations are the UIView or View (or windows equivalent) constructs that follow well defined points on the map.  This is how most users think of as annotations.  They work well enough, but add too many and they're slow as they're being dragged around by MapLibre.
- GeoJSON.  Question: Does this go through the vector tile mechanism?
- Drawable Builder support was added by our team in redevelopment, but we never quite finished it.  The intent was a mechanism to let developers add real time geometry without having to use the local rendering SDK directly.
- Geometry annotation support is an older mechanism which was intended to allow developers to add their own individual features.  A bit like our Drawable Builder support, this was kind of shoehorned in at one point.

Geometry Annotations are a logical approach to just adding one of something and not having to convert it to GeoJSON first.  Most developers just use the GeoJSON option, but this one is available.

Figure: Add a Geometry Annotations example

These are somewhat restrictive in what they can do, as there's no style sheet interaction.

It's not a heavily used option and so we're not quite sure where to take it in the future.

### New Wide Vector Implementation

We discuss the Line Layer above and there's one detail of what it does that uses a lot of memory and takes quite a bit of time.  The Line Layer has to calculate the vertices for the widened line and pass those in to the Line Shader.  Those intersections need to be flexible enough to change as the width changes.  The math can get interesting, but suffice it to say that it's an intersection with some input bits.

GPUs have been accurate enough to calculate those intersections on the fly for quite some time.  Rather than precalculating each intersected vertex between two line segments, we could just do it in real time.

The advantage would be that it could use less memory when rendering and load much faster.  We'd only need to pass in a center line and... well then it gets interesting.

We did this in WhirlyGlobe and made a junction between line segments we could stamp down as instances.  Thus we encoded a handful of vertices and instanced those for every center point along a centerline.

In MapLibre we added this for Custom Layers, but did not fully replace the existing Line Layer.  Since the Line Layer worked and was not in the critical path (it's usually Fill or Symbol that is) it wasn't urgent.  It would be interesting to revisit.

